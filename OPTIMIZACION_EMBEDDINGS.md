# âš¡ OptimizaciÃ³n de Embeddings - Singleton Pattern

## ğŸ¯ Problema Identificado

Antes de esta optimizaciÃ³n, el agente tenÃ­a una **latencia de ~7 segundos adicionales** en cada mensaje:
- **Nodo 3 (RecuperaciÃ³n EpisÃ³dica)**: ~4 segundos cargando el modelo
- **Nodo 7 (Persistencia EpisÃ³dica)**: ~3 segundos cargando el modelo

**Causa:** El modelo `paraphrase-multilingual-MiniLM-L12-v2` (384 dims, ~120MB) se cargaba desde disco en **cada invocaciÃ³n**.

---

## âœ… SoluciÃ³n Implementada

### 1. **Singleton Thread-Safe** en `src/embeddings/local_embedder.py`

```python
# Variables globales con thread-safety
_model_instance: Optional[SentenceTransformer] = None
_model_lock = threading.Lock()
_model_loaded = False

def get_embedder() -> SentenceTransformer:
    """Carga el modelo UNA SOLA VEZ en memoria"""
    global _model_instance, _model_loaded
    
    # Double-checked locking
    if not _model_loaded:
        with _model_lock:
            if not _model_loaded:
                logger.info("ğŸš€ [INIT] Cargando modelo...")
                _model_instance = SentenceTransformer(...)
                _model_loaded = True
    
    return _model_instance
```

**CaracterÃ­sticas:**
- âœ… Thread-safe con `threading.Lock()`
- âœ… Double-checked locking para performance
- âœ… Log solo en primera carga
- âœ… FunciÃ³n `warmup_embedder()` para pre-carga

### 2. **Pre-carga en Startup** en `app.py`

```python
@app.on_event("startup")
async def startup_event():
    """Pre-carga el modelo al iniciar el servidor"""
    logger.info("ğŸš€ Iniciando servidor FastAPI...")
    logger.info("ğŸ“¦ Pre-cargando modelo de embeddings...")
    
    warmup_embedder()
    logger.info("âœ… Servidor listo - Modelo en memoria")
```

**Resultado:** Cuando llega el primer mensaje de WhatsApp, el modelo ya estÃ¡ "caliente" en RAM.

### 3. **ActualizaciÃ³n de Nodo 7** en `src/nodes/persistencia_episodica_node.py`

**Antes:**
```python
# âŒ Cargaba su propia instancia
embedding_model = None
def get_embedding_model():
    global embedding_model
    if embedding_model is None:
        embedding_model = SentenceTransformer(MODEL_NAME)
```

**DespuÃ©s:**
```python
# âœ… Usa el singleton centralizado
from src.embeddings.local_embedder import generate_embedding, is_model_loaded

# Uso directo
embedding = generate_embedding(resumen)
```

---

## ğŸ“Š Resultados Esperados

### Antes de la OptimizaciÃ³n
```
Usuario envÃ­a mensaje:
  â”œâ”€ Nodo 3: ~4000ms (carga modelo + genera embedding)
  â”œâ”€ Nodo 4: ~200ms
  â”œâ”€ Nodo 5: ~1000ms
  â”œâ”€ Nodo 6: ~500ms
  â””â”€ Nodo 7: ~3000ms (carga modelo + guarda embedding)
  
Total: ~8700ms (~9 segundos)
```

### DespuÃ©s de la OptimizaciÃ³n
```
Startup del servidor:
  â””â”€ Warmup: ~4000ms (UNA SOLA VEZ)

Usuario envÃ­a mensaje:
  â”œâ”€ Nodo 3: ~50ms (embedding instantÃ¡neo)
  â”œâ”€ Nodo 4: ~200ms
  â”œâ”€ Nodo 5: ~1000ms
  â”œâ”€ Nodo 6: ~500ms
  â””â”€ Nodo 7: ~50ms (embedding instantÃ¡neo)
  
Total: ~1800ms (~2 segundos)
```

**Ganancia:** ~7 segundos por mensaje = **4.8x mÃ¡s rÃ¡pido** ğŸš€

---

## ğŸ§ª VerificaciÃ³n

### Ejecutar Test de Singleton

```bash
python test_embeddings_singleton.py
```

**Salida esperada:**
```
ğŸš€ [INIT] Cargando modelo de embeddings en memoria...
âœ… Modelo cargado exitosamente en 3.84s
âš¡ Las siguientes invocaciones serÃ¡n instantÃ¡neas

InvocaciÃ³n #1: 45.23ms
InvocaciÃ³n #2: 38.91ms
InvocaciÃ³n #3: 42.17ms

âœ… EXCELENTE - Tiempo promedio: 42.10ms
âœ… ReducciÃ³n de ~4000ms a ~42ms = 95x mÃ¡s rÃ¡pido
```

### Logs en ProducciÃ³n

**Primera vez (startup):**
```
ğŸš€ Iniciando servidor FastAPI...
ğŸ“¦ Pre-cargando modelo de embeddings...
ğŸš€ [INIT] Cargando modelo de embeddings en memoria por primera y Ãºnica vez...
   ğŸ“¦ Modelo: paraphrase-multilingual-MiniLM-L12-v2
   ğŸ“ Dimensiones: 384
   ğŸ’» Dispositivo: CPU
âœ… Modelo cargado exitosamente en 3.92s
âš¡ Las siguientes invocaciones serÃ¡n instantÃ¡neas
```

**Mensajes subsiguientes:**
```
ğŸ“– [3] NODO_RECUPERACION_EPISODICA - Buscando memoria relevante
    ğŸ”¢ Generando embedding (384 dims)...
    âœ… Embedding generado: [0.1234, 0.5678, ...]
    # âš ï¸ NO aparece "Cargando modelo..."
```

---

## ğŸ” Debugging

### Verificar si el modelo estÃ¡ cargado

```python
from src.embeddings.local_embedder import is_model_loaded

if is_model_loaded():
    print("âœ… Modelo en memoria")
else:
    print("âš ï¸  Modelo NO cargado (se cargarÃ¡ bajo demanda)")
```

### Logs importantes

- âœ… **`[INIT] Cargando modelo...`** â†’ Solo debe aparecer UNA VEZ al inicio
- âŒ Si aparece mÃºltiples veces â†’ El singleton no estÃ¡ funcionando
- âš ï¸ **`Modelo no pre-cargado, se cargarÃ¡ bajo demanda`** â†’ Verifica el `@app.on_event("startup")`

---

## ğŸ›ï¸ ConfiguraciÃ³n

### Variables de entorno (opcional)

Si deseas forzar un modelo diferente o dispositivo:

```bash
# .env
EMBEDDING_MODEL=paraphrase-multilingual-MiniLM-L12-v2  # Modelo a usar
EMBEDDING_DEVICE=cpu  # cpu o cuda
```

### Desactivar pre-carga (no recomendado)

Si por alguna razÃ³n quieres desactivar el warmup:

```python
# En app.py, comenta:
# @app.on_event("startup")
# async def startup_event():
#     warmup_embedder()
```

**Nota:** El modelo se cargarÃ¡ bajo demanda en el primer mensaje (aÃ±adiendo ~4s de latencia).

---

## ğŸ“ Archivos Modificados

### Creados:
- âœ¨ `test_embeddings_singleton.py` - Test de verificaciÃ³n

### Modificados:
- ğŸ”§ `src/embeddings/local_embedder.py` - Singleton thread-safe + warmup
- ğŸ”§ `src/nodes/persistencia_episodica_node.py` - Usa singleton centralizado
- ğŸ”§ `app.py` - Pre-carga en startup event

### Sin cambios:
- âœ… `src/nodes/recuperacion_episodica_node.py` - Ya usaba el singleton correctamente

---

## ğŸ’¡ Mejores PrÃ¡cticas

### âœ… DO:
- Pre-cargar el modelo en el startup
- Usar `warmup_embedder()` antes del primer request
- Verificar logs para confirmar carga Ãºnica
- Ejecutar `test_embeddings_singleton.py` despuÃ©s de desplegar

### âŒ DON'T:
- No crear nuevas instancias de `SentenceTransformer` en otros nodos
- No llamar `get_embedder()` sin necesidad
- No desactivar el warmup en producciÃ³n

---

## ğŸš€ PrÃ³ximos Pasos (Opcional)

### Optimizaciones Adicionales

1. **CuantizaciÃ³n del Modelo**
   - Reducir de float32 a float16
   - Ganancia: ~50% menos RAM, ~20% mÃ¡s rÃ¡pido

2. **CachÃ© de Embeddings Frecuentes**
   - Cachear embeddings de queries comunes
   - Ejemplo: "Â¿QuÃ© eventos tengo hoy?" â†’ embedding pre-calculado

3. **Batching de Embeddings**
   - Procesar mÃºltiples textos en un batch
   - Ãštil si se procesan N mensajes simultÃ¡neos

---

## ğŸ“š Referencias

- [SentenceTransformers Documentation](https://www.sbert.net/)
- [FastAPI Startup Events](https://fastapi.tiangolo.com/advanced/events/)
- [Python Singleton Pattern](https://refactoring.guru/design-patterns/singleton/python/example)
- [Thread-Safe Singleton](https://python-patterns.guide/gang-of-four/singleton/)

---

## âœ… Checklist de VerificaciÃ³n

- [x] Singleton implementado con thread-safety
- [x] FunciÃ³n `warmup_embedder()` creada
- [x] Startup event configurado en `app.py`
- [x] Nodo 7 actualizado para usar singleton
- [x] Test de verificaciÃ³n creado
- [x] Logs verifican carga Ãºnica
- [x] DocumentaciÃ³n completa

**Estado:** âœ… **OptimizaciÃ³n Completa y Lista para ProducciÃ³n**
