# üìñ NODO 3: Recuperaci√≥n Epis√≥dica con Embeddings Locales

## üéØ Objetivo

El **Nodo 3** implementa **memoria epis√≥dica sem√°ntica** usando embeddings multiling√ºes locales. Permite al agente recordar conversaciones pasadas relevantes cuando el usuario cambia de tema o pregunta sobre interacciones previas.

---

## üèóÔ∏è Arquitectura

### Flujo de Activaci√≥n

```
Usuario cambia de tema (detectado por Nodo 2)
         ‚Üì
    [NODO 3]
         ‚Üì
1. Extrae √∫ltimo mensaje del usuario
2. Genera embedding de 384 dimensiones (local)
3. Busca episodios similares en pgvector (coseno)
4. Filtra por user_id
5. Retorna top 3 resultados (umbral 0.7)
6. Formatea contexto para el agente
         ‚Üì
    [NODO 4] ‚Üí Contin√∫a con herramientas
```

### Componentes Clave

#### 1. **Modelo de Embeddings** (`src/embeddings/local_embedder.py`)
```python
Modelo: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2
Dimensiones: 384
Idiomas: 50+ (incluyendo espa√±ol)
Dispositivo: CPU (PyTorch 2.10.0+cpu)
Patr√≥n: Singleton (carga √∫nica)
```

**Caracter√≠sticas:**
- ‚úÖ Optimizado para espa√±ol y multiling√ºismo
- ‚úÖ Vectores normalizados (b√∫squeda coseno eficiente)
- ‚úÖ Carga bajo demanda (no bloquea inicio)
- ‚úÖ R√°pido en CPU (~20ms por embedding)

#### 2. **Nodo de Recuperaci√≥n** (`nodo_recuperacion_episodica` en `graph_whatsapp.py`)
```python
def nodo_recuperacion_episodica(state: WhatsAppAgentState) -> Dict:
    """
    Busca episodios relevantes del pasado usando similitud sem√°ntica
    
    Input:
        - state['messages']: Historial de conversaci√≥n
        - state['user_id']: ID del usuario (filtrado)
    
    Output:
        - contexto_episodico: {
            'query_embedding_dim': 384,
            'episodios_recuperados': [...],  # Top 3
            'similitud_threshold': 0.7,
            'texto_formateado': "..."
          }
    """
```

---

## üî¢ Especificaciones T√©cnicas

### Embedding Generation

```python
from src.embeddings.local_embedder import generate_embedding

texto = "¬øQu√© citas ten√≠a pendientes la semana pasada?"
embedding = generate_embedding(texto)  # List[float], len=384
```

**Propiedades:**
- Entrada: String (cualquier longitud)
- Salida: Lista de 384 floats
- Normalizaci√≥n: L2 (norma euclidiana)
- Similitud: Producto punto = cosine similarity

### pgvector Query (Pendiente de Implementaci√≥n)

```sql
-- Esquema de tabla
CREATE TABLE memoria_episodica (
    id SERIAL PRIMARY KEY,
    user_id VARCHAR(255) NOT NULL,
    session_id VARCHAR(255) NOT NULL,
    resumen TEXT NOT NULL,
    embedding vector(384),  -- pgvector extension
    timestamp TIMESTAMPTZ DEFAULT NOW(),
    metadata JSONB
);

-- √çndice para b√∫squeda eficiente
CREATE INDEX idx_memoria_embedding ON memoria_episodica 
USING ivfflat (embedding vector_cosine_ops) 
WITH (lists = 100);

-- B√∫squeda de similitud
SELECT 
    id, 
    resumen, 
    timestamp,
    1 - (embedding <=> '[...]'::vector) AS similitud
FROM memoria_episodica
WHERE user_id = 'test_user_123'
  AND 1 - (embedding <=> '[...]'::vector) >= 0.7  -- Umbral
ORDER BY embedding <=> '[...]'::vector ASC
LIMIT 3;
```

**Operador `<=>`:**
- Distancia coseno en pgvector
- `1 - distancia = similitud`
- Rango: [0, 2] ‚Üí Similitud: [-1, 1]

---

## üìä Resultados de Pruebas

### Test 1: Carga del Modelo ‚úÖ
```
üì¶ Cargando modelo paraphrase-multilingual-MiniLM-L12-v2...
   ‚úì Modelo cargado en 3.34s (primera vez)
   ‚úì Dimensiones: 384
   ‚úì Tipo de datos: <class 'float'>
   ‚úì Primeros 5 valores: [0.0328, -0.0001, -0.0377, 0.0245, -0.0157]
```

### Test 2: Calidad Sem√°ntica (Espa√±ol) ‚úÖ
```
üìä Similitudes (coseno):
   '¬øQu√© reuniones tengo ma√±ana?' ‚Üî '¬øCu√°les son mis citas de ma√±ana?': 0.7281
   '¬øQu√© reuniones tengo ma√±ana?' ‚Üî '¬øCu√°l es el clima de hoy?': 0.3009
```

**Interpretaci√≥n:**
- **0.73**: Alta similitud ‚Üí Mismo dominio sem√°ntico (calendario)
- **0.30**: Baja similitud ‚Üí Dominios diferentes (calendario vs clima)
- Umbral recomendado: **0.7** para resultados relevantes

### Test 3: Flujo Completo ‚úÖ
```
üöÄ Conversaci√≥n que cambia de tema:
   1. Usuario: "Hola"
   2. Asistente: "¬°Hola! ¬øEn qu√© puedo ayudarte?"
   3. Usuario: "Quiero agendar una reuni√≥n para el lunes"
   4. Asistente: "Perfecto, ¬øa qu√© hora?"
   5. Usuario: "Espera, ¬øqu√© citas ten√≠a pendientes la semana pasada?" ‚Üê CAMBIO

üìä RESULTADO:
   ‚úì Nodo 2 detect√≥ cambio: True
   ‚úì Nodo 3 gener√≥ embedding: 384 dims
   ‚úì B√∫squeda simulada (pgvector pendiente)
   ‚úì Contexto formateado: "No hay antecedentes previos para este tema"
   ‚úì Flujo continu√≥ sin errores
```

### Test 4: Robustez ‚úÖ
```
üõ°Ô∏è  Caso edge: Mensaje vac√≠o
   ‚úì Nodo 3 no activado (esperado: pocos mensajes)
   ‚úì Sistema contin√∫a sin errores
   ‚úì Fallback funcional
```

---

## ‚öôÔ∏è Configuraci√≥n

### Variables de Estado

```python
WhatsAppAgentState:
    cambio_de_tema: bool          # Activador del Nodo 3
    contexto_episodico: Dict | None  # Salida del Nodo 3
```

### Par√°metros del Nodo

```python
SIMILITUD_THRESHOLD = 0.7  # Umbral de relevancia (70%)
TOP_K_RESULTADOS = 3       # M√°ximo de episodios recuperados
EMBEDDING_DIM = 384        # Dimensiones del vector
```

---

## üîÑ Integraci√≥n con Otros Nodos

### ‚¨ÖÔ∏è Input (desde Nodo 2)
```python
state['cambio_de_tema'] = True  # Se√±al para activar Nodo 3
```

### ‚û°Ô∏è Output (hacia Nodo 4)
```python
state['contexto_episodico'] = {
    'query_embedding_dim': 384,
    'episodios_recuperados': [
        {
            'resumen': 'Usuario agend√≥ cita m√©dica para el martes 21',
            'timestamp': '2024-01-16T10:30:00',
            'similitud': 0.89
        },
        {
            'resumen': 'Usuario cancel√≥ reuni√≥n con equipo de ventas',
            'timestamp': '2024-01-15T14:20:00',
            'similitud': 0.76
        }
    ],
    'similitud_threshold': 0.7,
    'texto_formateado': '''
üìã Contexto de conversaciones previas:

üïí 16 Ene, 10:30
   Usuario agend√≥ cita m√©dica para el martes 21
   (Similitud: 89%)

üïí 15 Ene, 14:20
   Usuario cancel√≥ reuni√≥n con equipo de ventas
   (Similitud: 76%)
    '''
}
```

### üîó Uso en Nodo 4 (Selecci√≥n de Herramientas)
```python
contexto = state.get('contexto_episodico')
if contexto and contexto['episodios_recuperados']:
    # Usar contexto hist√≥rico para seleccionar herramientas relevantes
    prompt = f"""
    Contexto hist√≥rico:
    {contexto['texto_formateado']}
    
    Usuario actual: {ultimo_mensaje}
    
    ¬øQu√© herramientas necesitas?
    """
```

---

## üöÄ Implementaci√≥n

### 1. Instalar Dependencias
```bash
pip install sentence-transformers torch numpy
```

### 2. Crear M√≥dulo de Embeddings
Archivo: `src/embeddings/local_embedder.py`
```python
from sentence_transformers import SentenceTransformer
from typing import List
import logging

_model_instance = None  # Singleton

def get_embedder() -> SentenceTransformer:
    global _model_instance
    if _model_instance is None:
        logging.info("üîß Cargando modelo de embeddings multiling√ºe...")
        _model_instance = SentenceTransformer(
            'paraphrase-multilingual-MiniLM-L12-v2',
            device='cpu'
        )
    return _model_instance

def generate_embedding(text: str) -> List[float]:
    model = get_embedder()
    embedding = model.encode(text, normalize_embeddings=True)
    return embedding.tolist()

def get_embedding_dimension() -> int:
    return 384
```

### 3. Implementar Nodo en Grafo
```python
from src.embeddings.local_embedder import generate_embedding

def nodo_recuperacion_episodica(state: WhatsAppAgentState) -> Dict:
    logger.info("üìñ [3] NODO_RECUPERACION_EPISODICA - Buscando episodios")
    
    try:
        # 1. Extraer √∫ltimo mensaje
        mensajes = state['messages']
        ultimo_msg = next(
            (m['content'] for m in reversed(mensajes) if m['role'] == 'user'),
            None
        )
        
        if not ultimo_msg:
            return {'contexto_episodico': None}
        
        # 2. Generar embedding
        embedding = generate_embedding(ultimo_msg)
        
        # 3. Buscar en pgvector
        resultados = buscar_episodios_similares(
            user_id=state['user_id'],
            query_embedding=embedding,
            top_k=3,
            threshold=0.7
        )
        
        # 4. Formatear contexto
        texto = formatear_contexto(resultados)
        
        return {
            'contexto_episodico': {
                'query_embedding_dim': 384,
                'episodios_recuperados': resultados,
                'similitud_threshold': 0.7,
                'texto_formateado': texto
            }
        }
        
    except Exception as e:
        logger.error(f"‚ùå Error en recuperaci√≥n epis√≥dica: {e}")
        return {
            'contexto_episodico': {
                'episodios_recuperados': [],
                'texto_formateado': "No hay antecedentes previos",
                'fallback': True,
                'error': str(e)
            }
        }
```

---

## üîí Fallback y Errores

### Estrategia de Fallback
1. **Modelo no carga:** Contin√∫a sin contexto epis√≥dico
2. **Embedding falla:** Retorna contexto vac√≠o
3. **pgvector no conecta:** Modo degradado (sin memoria)
4. **Query timeout:** Contin√∫a con herramientas default

### Logging Detallado
```python
logger.info("üìù Query: '√öltima pregunta del usuario...'")
logger.info("üî¢ Generando embedding local (384 dims)...")
logger.info("‚úì Embedding generado: 384 dimensiones")
logger.info("üîç Buscando en memoria epis√≥dica (pgvector)...")
logger.info("‚úì Encontrados 2 episodios relevantes")
```

---

## üìà M√©tricas de Rendimiento

### Tiempos Medidos (CPU)
- **Primera carga del modelo:** ~3.5s
- **Cargas subsiguientes:** ~0.01s (singleton)
- **Generaci√≥n de embedding:** ~20-30ms
- **B√∫squeda pgvector (estimado):** ~50-100ms
- **Total por llamada:** ~80-150ms

### Uso de Memoria
- **Modelo cargado:** ~120 MB
- **Embedding (384 floats):** ~1.5 KB
- **Impacto total:** ~150 MB RAM

---

## üõ†Ô∏è Pr√≥ximos Pasos

### 1. Conectar PostgreSQL + pgvector
```bash
# Instalar extensi√≥n
CREATE EXTENSION vector;

# Crear tabla
CREATE TABLE memoria_episodica (
    id SERIAL PRIMARY KEY,
    user_id VARCHAR(255),
    session_id VARCHAR(255),
    resumen TEXT,
    embedding vector(384),
    timestamp TIMESTAMPTZ DEFAULT NOW()
);
```

### 2. Implementar B√∫squeda Real
```python
import psycopg2

def buscar_episodios_similares(user_id, query_embedding, top_k=3, threshold=0.7):
    conn = psycopg2.connect(DATABASE_URL)
    cur = conn.cursor()
    
    cur.execute("""
        SELECT resumen, timestamp, 
               1 - (embedding <=> %s::vector) AS similitud
        FROM memoria_episodica
        WHERE user_id = %s
          AND 1 - (embedding <=> %s::vector) >= %s
        ORDER BY embedding <=> %s::vector ASC
        LIMIT %s
    """, (query_embedding, user_id, query_embedding, threshold, query_embedding, top_k))
    
    return cur.fetchall()
```

### 3. Guardar Embeddings en Nodo 7
```python
def nodo_persistencia_episodica(state: WhatsAppAgentState):
    resumen = state['resumen_actual']
    embedding = generate_embedding(resumen)
    
    # Guardar en BD
    guardar_episodio(
        user_id=state['user_id'],
        resumen=resumen,
        embedding=embedding
    )
```

---

## ‚úÖ Checklist de Implementaci√≥n

- [x] Instalar sentence-transformers
- [x] Crear m√≥dulo local_embedder.py
- [x] Implementar patr√≥n singleton
- [x] Integrar en nodo_recuperacion_episodica
- [x] Pruebas de calidad sem√°ntica (espa√±ol)
- [x] Pruebas de flujo completo
- [x] Manejo de errores y fallback
- [ ] Conectar PostgreSQL + pgvector
- [ ] Implementar b√∫squeda real
- [ ] Guardar embeddings en Nodo 7
- [ ] Monitoreo de latencia y cache

---

## üìö Referencias

- **Modelo:** [sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2](https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2)
- **pgvector:** [Documentation](https://github.com/pgvector/pgvector)
- **LangGraph:** [Conditional Edges](https://langchain-ai.github.io/langgraph/concepts/low_level/)

---

## üéâ Estado Actual

‚úÖ **NODO 3 COMPLETADO Y PROBADO**

- Embeddings multiling√ºes funcionando
- 384 dimensiones (compatible con pgvector)
- Optimizado para espa√±ol
- Fallback robusto
- Listo para integraci√≥n con BD

**Pr√≥ximo paso:** Conectar PostgreSQL con extensi√≥n pgvector para b√∫squeda real.
