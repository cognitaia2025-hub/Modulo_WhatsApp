"""
Nodo 6: GeneraciÃ³n de ResÃºmenes (AuditorÃ­a)

Este nodo actÃºa como "auditor" de la sesiÃ³n, transformando la conversaciÃ³n
cruda en una cÃ¡psula de conocimiento estructurado.

Responsabilidades:
- Extraer hechos, pendientes, perfil y estado
- Generar resumen ejecutivo (<100 palabras)
- Comportamiento especial para sesiÃ³n expirada (retomar hilo)
- Incluir timestamp de Mexicali
- Preparar texto para Nodo 7 (persistencia pgvector)
"""

import logging
from typing import Dict, List, Any
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from src.utils.time_utils import get_current_time
import os
from dotenv import load_dotenv

# âœ… Imports para memoria semÃ¡ntica
from langgraph.store.base import BaseStore
from src.memory import update_semantic_memory

load_dotenv()

logger = logging.getLogger(__name__)

# LLM principal para auditorÃ­a
llm_primary = ChatOpenAI(
    model="deepseek-chat",
    temperature=0.2,  # MÃ¡s preciso para resÃºmenes concisos
    max_tokens=120,   # LÃ­mite estricto: ~80-100 palabras
    api_key=os.getenv("DEEPSEEK_API_KEY"),
    base_url="https://api.deepseek.com/v1",
    timeout=15.0,  # Timeout reducido a 15 segundos
    max_retries=0  # âœ… Reintentos los maneja LangGraph
)

# Fallback: Claude Haiku 4.5 (anÃ¡lisis rÃ¡pido)
llm_fallback = ChatAnthropic(
    model="claude-3-5-haiku-20241022",
    temperature=0.2,
    max_tokens=120,
    api_key=os.getenv("ANTHROPIC_API_KEY"),
    timeout=15.0,
    max_retries=0
)

# Auditor con fallback automÃ¡tico
llm_auditor = llm_primary.with_fallbacks([llm_fallback])


def extraer_mensajes_relevantes(messages: List[Any]) -> str:
    """
    Extrae y formatea los mensajes de la conversaciÃ³n excluyendo ruido.
    
    Args:
        messages: Lista de mensajes del state
        
    Returns:
        String con conversaciÃ³n formateada
    """
    conversacion = []
    
    for msg in messages:
        # Extraer contenido segÃºn tipo de mensaje
        if hasattr(msg, 'content'):
            contenido = msg.content
        elif isinstance(msg, dict) and 'content' in msg:
            contenido = msg['content']
        else:
            contenido = str(msg)
        
        # Extraer rol
        if hasattr(msg, 'role'):
            rol = msg.role
        elif isinstance(msg, dict) and 'role' in msg:
            rol = msg['role']
        elif hasattr(msg, '__class__'):
            rol = 'ai' if 'AI' in msg.__class__.__name__ else 'user'
        else:
            rol = 'desconocido'
        
        # Filtrar mensajes vacÃ­os o muy cortos
        if contenido and len(contenido.strip()) > 2:
            conversacion.append(f"{rol.upper()}: {contenido}")
    
    return "\n".join(conversacion) if conversacion else ""


def construir_prompt_auditoria(
    conversacion: str,
    contexto_episodico: Any,
    sesion_expirada: bool
) -> str:
    """
    Construye el prompt de auditorÃ­a para el LLM segÃºn el contexto.
    
    Args:
        conversacion: ConversaciÃ³n formateada
        contexto_episodico: Memoria de conversaciones previas
        sesion_expirada: Si la sesiÃ³n fue interrumpida por timeout
        
    Returns:
        Prompt estructurado para el auditor
    """
    timestamp = get_current_time().format('dddd, DD [de] MMMM [de] YYYY [a las] HH:mm', locale='es')
    
    # Extraer contexto previo si existe
    contexto_previo = ""
    if contexto_episodico and isinstance(contexto_episodico, dict):
        if 'resumen' in contexto_episodico:
            contexto_previo = f"\nCONTEXTO PREVIO:\n{contexto_episodico['resumen']}\n"
    
    # InstrucciÃ³n especial para sesiÃ³n expirada
    enfoque_especial = ""
    if sesion_expirada:
        enfoque_especial = """
âš ï¸ IMPORTANTE: Esta sesiÃ³n fue interrumpida hace 24 horas por timeout.
Tu resumen debe permitir retomar la conversaciÃ³n EXACTAMENTE donde se quedÃ³.
Prioriza:
- Estado especÃ­fico de la tarea (completada/pendiente/interrumpida)
- Ãšltima peticiÃ³n del usuario que quedÃ³ sin resolver
- Contexto necesario para continuar sin repetir informaciÃ³n
"""
    
    prompt = f"""Resume esta conversaciÃ³n de WhatsApp en MÃXIMO 50 PALABRAS.

FECHA: {timestamp}
{contexto_previo}
CONVERSACIÃ“N:
{conversacion}
{enfoque_especial}
Incluye: (1) QuÃ© se hizo, (2) QuÃ© falta, (3) Estado (completada/pendiente).
Usa texto plano SIN formato markdown. SÃ© directo y conciso.

Si no hay info relevante, responde solo: "Sin cambios relevantes"

RESUMEN:"""
    
    return prompt


def nodo_generacion_resumen(state: Dict[str, Any], store: BaseStore) -> Dict[str, Any]:
    """
    Nodo 6: Genera resumen ejecutivo de la sesiÃ³n para memoria a largo plazo.

    Este nodo es la "auditorÃ­a" que transforma conversaciÃ³n cruda en
    conocimiento estructurado antes de persistir en pgvector.

    TambiÃ©n actualiza preferencias del usuario usando LLM structured output.

    Args:
        state: Estado del grafo con messages, contexto_episodico, sesion_expirada
        store: BaseStore con memoria semÃ¡ntica (inyectado por LangGraph)

    Returns:
        State actualizado con resumen_actual
    """
    logger.info("ğŸ“ [6] NODO_GENERACION_RESUMEN - Auditando sesiÃ³n")

    messages = state.get('messages', [])
    contexto_episodico = state.get('contexto_episodico')
    sesion_expirada = state.get('sesion_expirada', False)
    user_id = state.get('user_id', 'default_user')
    
    # Validar que hay suficientes mensajes para resumir
    if len(messages) < 2:
        logger.info("    âš ï¸  Pocos mensajes para generar resumen")
        return {
            **state,
            'resumen_actual': "Sin cambios relevantes"
        }
    
    # Extraer conversaciÃ³n relevante
    conversacion = extraer_mensajes_relevantes(messages)
    
    if not conversacion or len(conversacion.strip()) < 10:
        logger.info("    âš ï¸  No hay contenido relevante para resumir")
        return {
            **state,
            'resumen_actual': "Sin cambios relevantes"
        }
    
    logger.info(f"    ğŸ“‹ ConversaciÃ³n a auditar: {len(conversacion)} caracteres")
    logger.info(f"    {'â°' if sesion_expirada else 'âœ…'} Modo: {'RECUPERACIÃ“N (sesiÃ³n expirada)' if sesion_expirada else 'NORMAL'}")
    
    # Construir prompt de auditorÃ­a
    prompt = construir_prompt_auditoria(
        conversacion=conversacion,
        contexto_episodico=contexto_episodico,
        sesion_expirada=sesion_expirada
    )
    
    try:
        # Invocar LLM auditor
        logger.info("    ğŸ¤– Invocando auditor LLM...")
        respuesta = llm_auditor.invoke(prompt)
        resumen = respuesta.content.strip()
        
        # AÃ±adir timestamp al resumen
        timestamp = get_current_time().format('DD/MM/YYYY HH:mm', locale='es')
        resumen_con_fecha = f"[{timestamp}] {resumen}"
        
        logger.info(f"    âœ… Resumen generado: '{resumen[:80]}...'")
        logger.info(f"    ğŸ“Š Longitud: {len(resumen)} caracteres")

        # âœ… Actualizar preferencias del usuario automÃ¡ticamente
        try:
            logger.info("    ğŸ§  Actualizando preferencias del usuario...")
            update_semantic_memory(
                state=state,
                store=store,
                user_id=user_id,
                llm=llm_auditor  # Reutilizar el mismo LLM auditor
            )
            logger.info("    âœ… Preferencias procesadas")
        except Exception as pref_error:
            logger.error(f"    âŒ Error actualizando preferencias: {pref_error}")
            import traceback
            logger.error(traceback.format_exc())

        return {
            **state,
            'resumen_actual': resumen_con_fecha
        }
        
    except Exception as e:
        logger.error(f"    âŒ Error al generar resumen: {e}")
        
        # Fallback: Resumen bÃ¡sico sin LLM
        timestamp = get_current_time().format('DD/MM/YYYY HH:mm', locale='es')
        resumen_fallback = f"[{timestamp}] ConversaciÃ³n con {len(messages)} mensajes. Estado: {'SesiÃ³n expirada' if sesion_expirada else 'Activa'}."
        
        return {
            **state,
            'resumen_actual': resumen_fallback
        }


def nodo_generacion_resumen_wrapper(state: Dict[str, Any], store: BaseStore) -> Dict[str, Any]:
    """
    Wrapper para compatibilidad con LangGraph.
    Recibe store de LangGraph (REQUERIDO) y lo pasa al nodo principal.
    """
    return nodo_generacion_resumen(state, store)


if __name__ == "__main__":
    """
    Test standalone del nodo de auditorÃ­a.
    """
    print("\nğŸ§ª Test del Nodo 6: GeneraciÃ³n de ResÃºmenes\n")
    
    # Test 1: ConversaciÃ³n normal con agendamiento
    print("Test 1: ConversaciÃ³n con agendamiento")
    state_test1 = {
        'messages': [
            {'role': 'user', 'content': 'Necesito agendar una reuniÃ³n para maÃ±ana'},
            {'role': 'ai', 'content': 'Â¿A quÃ© hora te gustarÃ­a agendar?'},
            {'role': 'user', 'content': 'A las 3 de la tarde'},
            {'role': 'ai', 'content': 'Perfecto, agendÃ© tu reuniÃ³n para maÃ±ana a las 15:00'}
        ],
        'user_id': 'test_user',
        'session_id': 'session_test1',
        'contexto_episodico': None,
        'sesion_expirada': False
    }
    
    resultado1 = nodo_generacion_resumen(state_test1)
    print(f"âœ… Resumen: {resultado1['resumen_actual']}\n")
    
    # Test 2: SesiÃ³n expirada con tarea pendiente
    print("Test 2: SesiÃ³n expirada (recuperaciÃ³n)")
    state_test2 = {
        'messages': [
            {'role': 'user', 'content': 'Ponme una cita para el miÃ©rcoles pero dÃ©jame confirmar el lugar'},
            {'role': 'ai', 'content': 'Ok, Â¿a quÃ© hora?'},
            {'role': 'user', 'content': 'A las 10 am'}
        ],
        'user_id': 'test_user',
        'session_id': 'session_test2',
        'contexto_episodico': {
            'resumen': 'Usuario prefiere reuniones por la maÃ±ana'
        },
        'sesion_expirada': True
    }
    
    resultado2 = nodo_generacion_resumen(state_test2)
    print(f"âœ… Resumen: {resultado2['resumen_actual']}\n")
    
    # Test 3: ConversaciÃ³n sin contenido relevante
    print("Test 3: Sin contenido relevante")
    state_test3 = {
        'messages': [
            {'role': 'user', 'content': 'Hola'},
            {'role': 'ai', 'content': 'Â¡Hola! Â¿En quÃ© puedo ayudarte?'}
        ],
        'user_id': 'test_user',
        'session_id': 'session_test3',
        'contexto_episodico': None,
        'sesion_expirada': False
    }
    
    resultado3 = nodo_generacion_resumen(state_test3)
    print(f"âœ… Resumen: {resultado3['resumen_actual']}\n")
    
    print("ğŸ‰ Tests completados")
