"""
Nodo: Filtrado Inteligente con LLM

Clasifica mensajes de usuarios en:
- 'personal': Eventos de calendario personal
- 'medica': Solicitudes m√©dicas (solo doctores)
- 'chat': Conversaci√≥n casual
- 'solicitud_cita_paciente': Pacientes externos solo pueden pedir citas

Estrategia:
1. LLM (DeepSeek) clasifica el mensaje
2. Fallback autom√°tico a Claude si DeepSeek falla
3. Validaci√≥n post-LLM: Pacientes externos ‚Üí SOLO solicitud_cita_paciente
4. Auditor√≠a en tabla clasificaciones_llm
"""

import logging
import json
import time
from typing import Dict
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_core.messages import SystemMessage, HumanMessage
from dotenv import load_dotenv
import os
import psycopg

from src.state.agent_state import WhatsAppAgentState

load_dotenv()
logger = logging.getLogger(__name__)

# Configuraci√≥n de LLMs
DATABASE_URL = os.getenv("DATABASE_URL", "postgresql://postgres:postgres@localhost:5434/agente_whatsapp")

# LLM primario: DeepSeek
llm_primary = ChatOpenAI(
    model="deepseek-chat",
    temperature=0,
    max_tokens=200,
    api_key=os.getenv("DEEPSEEK_API_KEY"),
    base_url="https://api.deepseek.com/v1",
    timeout=30.0,
    max_retries=0
)

# LLM fallback: Claude
llm_fallback = ChatAnthropic(
    model="claude-3-5-haiku-20241022",
    temperature=0,
    max_tokens=200,
    api_key=os.getenv("ANTHROPIC_API_KEY"),
    timeout=20.0,
    max_retries=0
)

# LLM con fallback autom√°tico
llm_with_fallback = llm_primary.with_fallbacks([llm_fallback])


def construir_prompt_clasificacion(mensaje_usuario: str, tipo_usuario: str) -> list:
    """
    Construye prompt para clasificaci√≥n de mensajes
    
    Args:
        mensaje_usuario: Mensaje del usuario
        tipo_usuario: Tipo de usuario ('doctor', 'paciente_externo', etc.)
        
    Returns:
        Lista de mensajes para el LLM
    """
    system_prompt = """Eres un clasificador de mensajes m√©dicos. Tu trabajo es clasificar mensajes en EXACTAMENTE una de estas categor√≠as:

CATEGOR√çAS PERMITIDAS:
1. "personal" - Eventos de calendario personal (cumplea√±os, reuniones personales, recordatorios no m√©dicos)
2. "medica" - Solicitudes m√©dicas de un doctor (revisar pacientes, agendar, historiales)
3. "chat" - Conversaci√≥n casual (saludos, despedidas, chat general)
4. "solicitud_cita_paciente" - Paciente externo pide cita m√©dica

REGLAS ESTRICTAS:
- Responde √öNICAMENTE con un JSON v√°lido
- Formato: {"clasificacion": "CATEGORIA", "confianza": 0.95, "razonamiento": "breve explicaci√≥n"}
- NO agregues texto fuera del JSON
- confianza debe ser n√∫mero entre 0.0 y 1.0

EJEMPLOS:

Usuario dice: "Quiero una cita m√©dica"
Respuesta: {"clasificacion": "solicitud_cita_paciente", "confianza": 0.95, "razonamiento": "Paciente solicita cita"}

Usuario dice (doctor): "¬øC√≥mo est√° mi paciente Juan?"
Respuesta: {"clasificacion": "medica", "confianza": 0.98, "razonamiento": "Doctor pregunta por paciente"}

Usuario dice: "Hola buenos d√≠as"
Respuesta: {"clasificacion": "chat", "confianza": 0.99, "razonamiento": "Saludo casual"}

Usuario dice: "Recu√©rdame el cumplea√±os de Mar√≠a"
Respuesta: {"clasificacion": "personal", "confianza": 0.97, "razonamiento": "Evento personal"}"""

    user_prompt = f"""Mensaje del usuario: "{mensaje_usuario}"
Tipo de usuario: {tipo_usuario}

Clasifica este mensaje en la categor√≠a correcta. Responde SOLO con el JSON."""

    return [
        SystemMessage(content=system_prompt),
        HumanMessage(content=user_prompt)
    ]


def parsear_respuesta_llm(respuesta: str) -> Dict:
    """
    Parsea respuesta del LLM a diccionario
    
    Args:
        respuesta: String del LLM (JSON esperado)
        
    Returns:
        Dict con clasificacion, confianza, razonamiento
    """
    try:
        # Limpiar respuesta
        respuesta = respuesta.strip()
        
        # Extraer JSON si est√° envuelto en markdown
        if "```json" in respuesta:
            respuesta = respuesta.split("```json")[1].split("```")[0].strip()
        elif "```" in respuesta:
            respuesta = respuesta.split("```")[1].split("```")[0].strip()
        
        # Parsear JSON
        resultado = json.loads(respuesta)
        
        # Validar campos requeridos
        if "clasificacion" not in resultado:
            raise ValueError("Falta campo 'clasificacion'")
        
        # Valores por defecto
        return {
            "clasificacion": resultado["clasificacion"],
            "confianza": resultado.get("confianza", 0.8),
            "razonamiento": resultado.get("razonamiento", "Sin razonamiento")
        }
    
    except (json.JSONDecodeError, ValueError) as e:
        logger.error(f"‚ùå Error parseando respuesta LLM: {e}")
        logger.error(f"    Respuesta recibida: {respuesta}")
        
        # Fallback: clasificar como chat
        return {
            "clasificacion": "chat",
            "confianza": 0.5,
            "razonamiento": "Error en parseo, clasificado como chat por defecto"
        }


def validar_clasificacion_por_tipo_usuario(
    clasificacion: str,
    tipo_usuario: str
) -> str:
    """
    Valida que la clasificaci√≥n sea compatible con el tipo de usuario
    
    Regla cr√≠tica: Pacientes externos SOLO pueden tener 'solicitud_cita_paciente'
    
    Args:
        clasificacion: Clasificaci√≥n del LLM
        tipo_usuario: Tipo de usuario
        
    Returns:
        Clasificaci√≥n validada (puede ser modificada)
    """
    # Pacientes externos: SOLO solicitud_cita_paciente o chat
    if tipo_usuario == "paciente_externo":
        if clasificacion in ["medica", "personal"]:
            logger.warning(
                f"‚ö†Ô∏è  Paciente externo clasificado como '{clasificacion}', "
                f"corrigiendo a 'solicitud_cita_paciente'"
            )
            return "solicitud_cita_paciente"
    
    # Doctores: todas las clasificaciones permitidas
    return clasificacion


def registrar_clasificacion_bd(
    user_phone: str,
    mensaje: str,
    clasificacion: str,
    confianza: float,
    modelo_usado: str,
    tipo_usuario: str,
    tiempo_ms: int,
    hubo_fallback: bool,
    razon_fallback: str = None
):
    """
    Registra clasificaci√≥n en la base de datos para auditor√≠a
    
    Args:
        user_phone: Tel√©fono del usuario
        mensaje: Mensaje clasificado
        clasificacion: Clasificaci√≥n asignada
        confianza: Nivel de confianza (0.0-1.0)
        modelo_usado: Modelo LLM usado
        tipo_usuario: Tipo de usuario
        tiempo_ms: Tiempo de procesamiento en ms
        hubo_fallback: Si se us√≥ fallback
        razon_fallback: Raz√≥n del fallback
    """
    try:
        with psycopg.connect(DATABASE_URL) as conn:
            with conn.cursor() as cur:
                cur.execute("""
                    INSERT INTO clasificaciones_llm (
                        user_phone,
                        mensaje_usuario,
                        clasificacion,
                        confianza,
                        modelo_usado,
                        tipo_usuario,
                        tiempo_procesamiento_ms,
                        hubo_fallback,
                        razon_fallback
                    ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)
                """, (
                    user_phone,
                    mensaje[:500],  # Limitar tama√±o
                    clasificacion,
                    confianza,
                    modelo_usado,
                    tipo_usuario,
                    tiempo_ms,
                    hubo_fallback,
                    razon_fallback
                ))
                conn.commit()
        
        logger.info(f"‚úÖ Clasificaci√≥n registrada en BD: {clasificacion}")
    
    except Exception as e:
        logger.error(f"‚ùå Error registrando clasificaci√≥n: {e}")
        # No fallar el flujo principal si falla el registro


def nodo_filtrado_inteligente(state: WhatsAppAgentState) -> Dict:
    """
    Nodo de filtrado inteligente con LLM
    
    Flujo:
    1. Extrae √∫ltimo mensaje del usuario
    2. Construye prompt de clasificaci√≥n
    3. Llama a LLM (DeepSeek con fallback a Claude)
    4. Parsea respuesta
    5. Valida seg√∫n tipo de usuario
    6. Registra en BD para auditor√≠a
    7. Actualiza state
    
    Args:
        state: WhatsAppAgentState
        
    Returns:
        Dict con actualizaciones del state
    """
    logger.info("\n" + "=" * 70)
    logger.info("üîç NODO: FILTRADO INTELIGENTE")
    logger.info("=" * 70)
    
    inicio = time.time()
    
    # Extraer datos del state
    messages = state.get("messages", [])
    tipo_usuario = state.get("tipo_usuario", "paciente_externo")
    user_id = state.get("user_id", "unknown")
    
    # Extraer √∫ltimo mensaje
    ultimo_mensaje = ""
    for msg in reversed(messages):
        if hasattr(msg, "type") and msg.type == "human":
            ultimo_mensaje = msg.content
            break
        elif isinstance(msg, dict) and msg.get("role") == "user":
            ultimo_mensaje = msg.get("content", "")
            break
    
    if not ultimo_mensaje:
        logger.warning("‚ö†Ô∏è  No se encontr√≥ mensaje del usuario")
        return {
            "clasificacion_mensaje": "chat",
            "confianza_clasificacion": 0.5
        }
    
    logger.info(f"üìù Mensaje: {ultimo_mensaje[:100]}...")
    logger.info(f"üë§ Tipo usuario: {tipo_usuario}")
    
    # Construir prompt
    prompt_messages = construir_prompt_clasificacion(ultimo_mensaje, tipo_usuario)
    
    # Llamar a LLM con fallback
    modelo_usado = "deepseek"
    hubo_fallback = False
    razon_fallback = None
    
    try:
        logger.info("ü§ñ Llamando a DeepSeek...")
        respuesta_llm = llm_primary.invoke(prompt_messages)
        modelo_usado = "deepseek"
    
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è  DeepSeek fall√≥: {e}")
        logger.info("üîÑ Intentando con Claude (fallback)...")
        
        try:
            respuesta_llm = llm_fallback.invoke(prompt_messages)
            modelo_usado = "claude"
            hubo_fallback = True
            razon_fallback = str(e)[:200]
        
        except Exception as e2:
            logger.error(f"‚ùå Ambos LLMs fallaron: {e2}")
            
            # Fallback final: clasificar como chat
            return {
                "clasificacion_mensaje": "chat",
                "confianza_clasificacion": 0.3
            }
    
    # Parsear respuesta
    resultado = parsear_respuesta_llm(respuesta_llm.content)
    
    clasificacion = resultado["clasificacion"]
    confianza = resultado["confianza"]
    razonamiento = resultado["razonamiento"]
    
    logger.info(f"üìä Clasificaci√≥n: {clasificacion}")
    logger.info(f"üíØ Confianza: {confianza}")
    logger.info(f"üí≠ Razonamiento: {razonamiento}")
    
    # Validar seg√∫n tipo de usuario
    clasificacion_validada = validar_clasificacion_por_tipo_usuario(
        clasificacion,
        tipo_usuario
    )
    
    if clasificacion != clasificacion_validada:
        logger.warning(
            f"‚ö†Ô∏è  Clasificaci√≥n ajustada: {clasificacion} ‚Üí {clasificacion_validada}"
        )
        clasificacion = clasificacion_validada
    
    # Calcular tiempo
    tiempo_ms = int((time.time() - inicio) * 1000)
    logger.info(f"‚è±Ô∏è  Tiempo: {tiempo_ms}ms")
    
    # Registrar en BD
    registrar_clasificacion_bd(
        user_phone=user_id,
        mensaje=ultimo_mensaje,
        clasificacion=clasificacion,
        confianza=confianza,
        modelo_usado=modelo_usado,
        tipo_usuario=tipo_usuario,
        tiempo_ms=tiempo_ms,
        hubo_fallback=hubo_fallback,
        razon_fallback=razon_fallback
    )
    
    logger.info("‚úÖ Filtrado inteligente completado\n")
    
    # Retornar actualizaciones del state
    return {
        "clasificacion_mensaje": clasificacion,
        "confianza_clasificacion": confianza,
        "modelo_clasificacion_usado": modelo_usado,
        "tiempo_clasificacion_ms": tiempo_ms
    }


# Wrapper para compatibilidad con grafo
def nodo_filtrado_inteligente_wrapper(state: WhatsAppAgentState) -> WhatsAppAgentState:
    """
    Wrapper que mantiene la firma esperada por el grafo
    """
    try:
        # Llamar al nodo principal
        resultado = nodo_filtrado_inteligente(state)
        
        # Retornar solo las actualizaciones del estado (no el estado completo)
        return {
            "clasificacion_mensaje": resultado.get("clasificacion_mensaje", "chat"),
            "confianza_clasificacion": resultado.get("confianza_clasificacion", 0.0),
            "modelo_clasificacion_usado": resultado.get("modelo_clasificacion_usado", "fallback"),
            "tiempo_clasificacion_ms": resultado.get("tiempo_clasificacion_ms", 0)
        }
        
    except Exception as e:
        logger.error(f"‚ùå Error en nodo filtrado inteligente: {e}")

        # Respuesta de fallback
        return {
            "clasificacion_mensaje": "chat",
            "confianza_clasificacion": 0.0,
            "modelo_clasificacion_usado": "fallback_error",
            "tiempo_clasificacion_ms": 0
        }
